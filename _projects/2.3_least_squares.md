---
title: 2.3 Least Squares Method
parent: 2. Linear Optics Measurements
nav_order: 3
layout: default
---

# Least Squares
Least Squares Methods minimize the distances between a linear equation and the points of a dataset:

$$Y = MX + B + \sigma_y$$

Where X and Y are points of the dataset retrieved from experiment/ simulation, and M and B are the parameters to fit the dataset. $$\sigma_y$$ is any errors that come from $$Y$$. We call this regression method Ordinary Least Squares (OLS).

## Ordinary Least Squares (OLS)

OLS can be a single or multi-variable method. For example, if Y depends on a single variable, $$X$$ and $$Y$$ are vectors and $$M, B$$, and $$\sigma_y$$ are scalars. For the general case, $$X$$ and $$Y$$ will have the same shape that is $$m \times n \times N$$, where $$m \times n$$ is the number of vectors for OLS and $$N$$ is the length of each vector. $$M, B$$, and $$\sigma_y$$ subsequently has shape $$m \times n$$ associated with each vector, i.e., every slope, y-intercept, and error is associated with every vector of $$X$$.

Multivariate case:
$$  
\begin{pmatrix} Y^{11} && Y^{12} \\ 
                Y^{21} && Y^{22} \end{pmatrix} =
\begin{pmatrix} M^{11} && M^{12} \\ 
                M^{21} && M^{22} \end{pmatrix} \cdot
\begin{pmatrix} X^{11} && X^{12} \\ 
                X^{21} && X^{22} \end{pmatrix} +
\begin{pmatrix} B^{11} && B^{12} \\ 
                B^{21} && B^{22} \end{pmatrix} +
\begin{pmatrix} \sigma^{11}_y && \sigma^{12}_y \\ 
                \sigma^{21}_y && \sigma^{22}_y \end{pmatrix}
$$

Where:

$$ 
Y^{mn}_i = \begin{pmatrix} Y^{mn}_1 && Y^{mn}_2 && ... && Y^{mn}_N \end{pmatrix}
$$

$$
X^{mn}_i = \begin{pmatrix} X^{mn}_1 && X^{mn}_2 && ... && X^{mn}_N \end{pmatrix}
$$

For the least squares equation, the mean of the dataset is a standard guess for $$B$$ and is subtracted out. The solution to the OLS regression in matrix form is therefore:

$$M = (X^TX)^{-1}X^TY$$

There are certian assumptions when using OLS:
1. Linearity: $$Y$$ can be made from a linear combination of all independent variables $$X$$.
2. Independence of errors: All errors are independent from each other and of the independent variables themselves.
3. No Co-linearity: All independent variables are independent from each other.
4. Homoscedasticity: Assumes the variance between variables in $$Y$$ ($$\sigma_y^2$$) remains constant for all values of the independent variables $$X$$
5. Errors-in-variables: The independent variables $$X$$ contain no errors, or that those errors that are much smaller than the range of $$X$$ values that can be attained.

## Total Least Squares

Do 1-D case first

<hr>

<script src="https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js"></script>
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>

<div id="LS-plot" style="width: 100%; height: 600px;"></div>

<script src="/assets/js/2_linear_optics_measurements/least_squares.js"></script>

<div style="max-width: 600px;">
    <label>
        \( \sigma_x \):
        <input id="xn-input" type="range" min="0.0" max="2.0" step="0.1" value="0.5">
        <span id="xn-value">0.5</span>
    </label>
    <br>
    <label>
        \( \sigma_y \):
        <input id="yn-input" type="range" min="0.0" max="2.0" step="0.1" value="0.5">
        <span id="yn-value">0.5</span>
    </label>
    <br>
    <label>
        True Slope (\( m \)):
        <input id="m-input" type="range" min="-5.0" max="5.0" step="0.1" value="2.0">
        <span id="m-value">2.0</span>
    </label>
    <br>
    <label>
        Number of Particles:
        <input id="N-input" type="range" min="100" max="1000" step="10" value="200">
        <span id="N-value">200</span>
    </label>
    <br>
    <button id="reset-button" style="margin-top: 10px;">
    RESET
    </button>
<div>

<hr />


# Least Squares for BPM Measurements
Might be biased for RHIC collider?
## Beta vs Phase
